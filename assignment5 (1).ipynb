{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb77272c-6852-4378-b69b-a81abe1b3e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "  #Answer: 1\n",
    "    \n",
    "A clustering result satisfies homogeneity if all of its clusters contain only data points which are members of a \n",
    "single class. A clustering result satisfies completeness if all the data points that are members of a given class\n",
    "are elements of the same cluster.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20700ce5-0608-4c92-8fdc-41f0a8987ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "  #Answer: 2\n",
    "    \n",
    "This score is a measure between 0–1 that actually quantifies the goodness of the clustering partition. \n",
    "In fact, it requires that both homogeneity h and completeness c are maximised (NMI is 1 when both h and c are 1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d49a807-31c9-4049-bb48-1d048dc3dfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "  #Answer: 3\n",
    "   \n",
    "The value of the silhouette coefficient is between −1 and 1. \n",
    "The value of a(o) reflects the compactness of the cluster to which o belongs.\n",
    "The smaller the value, the more compact the cluster. \n",
    "The value of b(o) captures the degree to which o is separated from other clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808121df-ba51-4d02-a741-1a5633a9d3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "  #Answer: 4\n",
    "    \n",
    "Davies-Bouldin Index (0.66): This index calculates the average similarity between each cluster and its closest\n",
    "neighbors. A lower score is preferable, and 0.66 suggests a pretty strong separation across clusters.\n",
    "The score Index (561.63) calculates the ratio of between-cluster variation to within-cluster variance.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6b3fa9-9348-4733-b49e-4bef5d753676",
   "metadata": {},
   "outputs": [],
   "source": [
    "  #Answer: 5\n",
    "    \n",
    "The homogenity and complete- ness of a clustering solution run roughly in opposi- tion: Increasing the\n",
    "homogeneity of a clustering so- lution often results in decreasing its completeness.\n",
    "Consider, two degenerate clustering solutions    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6ed3e5-38ed-43e9-94a4-8e8a4aa7cbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "  #Answer: 6\n",
    "    \n",
    "The V-measure is a clustering evaluation metric that combines both homogeneity and completeness to provide a single measure of clustering quality. It can be used to determine the optimal number of clusters in a clustering algorithm by comparing the V-measure scores for different numbers of clusters. \n",
    "\n",
    "Here's how the V-measure can be used:\n",
    "\n",
    "1. **Compute V-measure for Different Numbers of Clusters**: Apply the clustering algorithm with varying numbers of clusters (e.g., 2, 3, 4, ...) to the dataset and compute the V-measure for each clustering result.\n",
    "\n",
    "2. **Plot V-measure Scores**: Plot the V-measure scores against the number of clusters. This will create a curve showing how the V-measure changes as the number of clusters increases.\n",
    "\n",
    "3. **Identify the Elbow Point**: Look for the point on the plot where the increase in V-measure starts to diminish (i.e., the elbow point). This point represents the optimal number of clusters where adding more clusters does not significantly improve the clustering quality.\n",
    "\n",
    "4. **Select the Optimal Number of Clusters**: Based on the elbow point, select the optimal number of clusters that provides a good balance between homogeneity and completeness.\n",
    "\n",
    "By using the V-measure in this way, you can objectively determine the optimal number of clusters for your dataset. However, it's essential to consider other factors such as domain knowledge, interpretability, and computational constraints when selecting the number of clusters. Additionally, it's recommended to try multiple clustering algorithms and compare their results using the V-measure to ensure robustness.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0711016-8c8e-466e-8db5-b178a7f48aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "  #Answer: 7\n",
    "\n",
    "2 Silhouette method\n",
    "The advantage of this method is that it can capture the compactness and separation of clusters better than the SSE. \n",
    "The disadvantage is that it can be computationally expensive and sensitive to outliers.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e8bf29-b9af-4194-9f6e-1d401e1c0d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "  #Answer: 8\n",
    "    \n",
    "The Davies-Bouldin Index (DBI) is not without its drawbacks, however. It can be sensitive to outliers and noise,\n",
    "leading to a false indication of poor clustering. Furthermore, it assumes a spherical shape with similar sizes and \n",
    "densities for each cluster, which may not be true in many real-world cases.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a13539-572b-4328-a980-aa0639ceb317",
   "metadata": {},
   "outputs": [],
   "source": [
    "  #Answer: 9\n",
    "    \n",
    "The homogenity and complete- ness of a clustering solution run roughly in opposi- tion: \n",
    "Increasing the homogeneity of a clustering so- lution often results in decreasing its completeness.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1955e36-188b-4d4c-8eef-8cf71efc5e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "  #Answer: 10\n",
    "    \n",
    "The silhouette score is a metric used to evaluate the quality of clusters created by clustering algorithms, \n",
    "like K-means, hierarchical clustering, etc. \n",
    "It measures how similar an object is to its own cluster (cohesion) compared to other clusters (separation).    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695de0a4-3725-42a1-8090-d0d9f569b9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "  #Answer: 11\n",
    "    \n",
    "It is calculated as the average similarity measure of each cluster with the cluster most similar to it. \n",
    "In this context, similarity is defined as the ratio between inter-cluster and intra-cluster distances. \n",
    "As such, this index ranks well-separated clusters with less dispersion as having a better score.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9ecad8-2db1-45d4-80a6-37e1ae3513f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "  #Answer: 12\n",
    "    \n",
    "The two most popular evaluation metrics for clustering algorithms are the Silhouette coefficient and Dunn's Index,\n",
    "which you will explore next. The Silhouette Coefficient is defined for each sample and is composed of two scores: a: \n",
    "The mean distance between a sample and all other points in the same cluster.    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
